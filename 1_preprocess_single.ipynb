{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import tarfile\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from six.moves import cPickle as pickle\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "last_percent_reported = None\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"\n",
    "    A hook to report the progress of a download. This is mostly intended for users with\n",
    "    slow internet connections. Reports every 1% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, force=False):\n",
    "    \"\"\"\n",
    "    Download a file if not present, and make sure it's the right size.\n",
    "    \"\"\"\n",
    "    if force or not os.path.exists(filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "        \n",
    "    statinfo = os.stat(filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = maybe_download('train.tar.gz')\n",
    "test_filename = maybe_download('test.tar.gz')\n",
    "\n",
    "train_matfile = maybe_download('train_32x32.mat')\n",
    "test_matfile = maybe_download('test_32x32.mat')\n",
    "extra_matfile = maybe_download('extra_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = scipy.io.loadmat('train_32x32.mat', variable_names='X').get('X')\n",
    "train_labels = scipy.io.loadmat('train_32x32.mat', variable_names='y').get('y')\n",
    "test_data = scipy.io.loadmat('test_32x32.mat', variable_names='X').get('X')\n",
    "test_labels = scipy.io.loadmat('test_32x32.mat', variable_names='y').get('y')\n",
    "extra_data = scipy.io.loadmat('extra_32x32.mat', variable_names='X').get('X')\n",
    "extra_labels = scipy.io.loadmat('extra_32x32.mat', variable_names='y').get('y')\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)\n",
    "print(extra_data.shape, extra_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace 10 with 0 for 0 digit\n",
    "train_labels[train_labels == 10] = 0\n",
    "test_labels[test_labels == 10] = 0\n",
    "extra_labels[extra_labels == 10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build Validation Dataset and Labels Based on the Methods in This Paper:\n",
    "# [https://arxiv.org/pdf/1204.3968.pdf]\n",
    "\n",
    "random.seed()\n",
    "\n",
    "# Number of classes\n",
    "n_labels = 10\n",
    "\n",
    "valid_index = []\n",
    "train_index = []\n",
    "extra_valid_index = []\n",
    "extra_train_index = []\n",
    "\n",
    "\n",
    "# Here we collect indicies of each class. \n",
    "# The validation set will consis of 2/3(400) entries from training set\n",
    "# And 1/3(200) entries from extra set, which contains easy samples for each class\n",
    "for i in np.arange(n_labels):\n",
    "    valid_index.extend(np.where(train_labels[:,0] == (i)) [0][:400].tolist())\n",
    "    train_index.extend(np.where(train_labels[:,0] == (i)) [0][400:].tolist())\n",
    "    \n",
    "    extra_valid_index.extend(np.where(extra_labels[:,0] == (i))[0][:200].tolist())\n",
    "    extra_train_index.extend(np.where(extra_labels[:,0] == (i))[0][200:].tolist())\n",
    "\n",
    "# Since we don't know much about how data is collected we shuffle these indicies.\n",
    "random.shuffle(valid_index)\n",
    "random.shuffle(train_index)\n",
    "random.shuffle(extra_valid_index)\n",
    "random.shuffle(extra_train_index)\n",
    "\n",
    "# Now we combine and transpose data from training and extra set.\n",
    "valid_data = np.concatenate((extra_data[:,:,:,extra_valid_index], train_data[:,:,:,valid_index]), axis=3).transpose((3,0,1,2))\n",
    "valid_labels = np.concatenate((extra_labels[extra_valid_index,:], train_labels[valid_index,:]), axis=0)[:,0]\n",
    "\n",
    "train_data_t = np.concatenate((extra_data[:,:,:,extra_train_index], train_data[:,:,:,train_index]), axis=3).transpose((3,0,1,2))\n",
    "train_labels_t = np.concatenate((extra_labels[extra_train_index,:], train_labels[train_index,:]), axis=0)[:,0]\n",
    "\n",
    "test_data = test_data.transpose((3,0,1,2))\n",
    "test_labels = test_labels[:,0]\n",
    "\n",
    "print(train_data_t.shape, train_labels_t.shape)\n",
    "print(test_data.shape, test_labels.shape)\n",
    "print(valid_data.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 32  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def make_gray(image):\n",
    "    '''\n",
    "    Normalize images\n",
    "    '''\n",
    "    image = image.astype(float)\n",
    "    image_gray = np.dot(image, [[0.2989],[0.5870],[0.1140]])\n",
    "    return image_gray\n",
    "\n",
    "gray_train_data = make_gray(train_data_t)[:,:,:,0]\n",
    "gray_test_data = make_gray(test_data)[:,:,:,0]\n",
    "gray_valid_data = make_gray(valid_data)[:,:,:,0]\n",
    "\n",
    "print(train_data_c.shape, train_labels_t.shape)\n",
    "print(test_data_c.shape, test_labels.shape)\n",
    "print(valid_data_c.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GCN(image, min_divisor=1e-4):\n",
    "    \"\"\"Global Contrast Normalization\"\"\"\n",
    "    \n",
    "    imsize = image.shape[0]\n",
    "    mean = np.mean(image, axis=(1,2), dtype=float)\n",
    "    std = np.std(image, axis=(1,2), dtype=float, ddof=1)\n",
    "    std[std < min_divisor] = 1.\n",
    "    image_GCN = np.zeros(image.shape, dtype=float)\n",
    "    \n",
    "    for i in np.arange(imsize):\n",
    "        image_GCN[i,:,:] = (image[i,:,:] - mean[i]) / std[i]\n",
    "        \n",
    "    return image_GCN\n",
    "\n",
    "train_data_GCN = GCN(train_data_c)\n",
    "test_data_GCN = GCN(test_data_c)\n",
    "valid_data_GCN = GCN(valid_data_c)\n",
    "\n",
    "print(train_data_GCN.shape, train_labels_t.shape)\n",
    "print(test_data_GCN.shape, test_labels.shape)\n",
    "print(valid_data_GCN.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "f, ax = plt.subplots(nrows=1, ncols=10)\n",
    "\n",
    "for i, j in enumerate(np.random.randint(0, train_labels_t.shape[0], size=10)):\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(train_labels_t[j], loc='center')\n",
    "    ax[i].imshow(train_data_GCN[j,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        #'train_dataset': train_data_GCN,\n",
    "        'train_labels': train_labels_t,\n",
    "        'valid_dataset': valid_data_GCN,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_data_GCN,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN1.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset1': train_data_GCN[:200000],\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN2.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset2': train_data_GCN[200000:400000],\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN3.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset3': train_data_GCN[400000:],\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Logistic Regression Classifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, shuffle=True, verbose=0,\\\n",
    "                    n_jobs=4, random_state=None, learning_rate='optimal')\n",
    "clf.fit(train_data_c.reshape(train_data_GCN.shape[0],-1), train_labels_t)\n",
    "train_prediction = clf.predict(train_data_GCN.reshape(train_data_GCN.shape[0],-1))\n",
    "valid_prediction = clf.predict(valid_data_GCN.reshape(valid_data_GCN.shape[0],-1))\n",
    "\n",
    "print('Training score is', clf.score(train_data_GCN.reshape(train_data_GCN.shape[0],-1), train_labels_t))\n",
    "print('Validation score is', clf.score(valid_data_GCN.reshape(valid_data_GCN.shape[0],-1), valid_labels))\n",
    "\n",
    "print('Classification report of training data:\\n', classification_report(train_labels_t, train_prediction))\n",
    "print('Confusion Matrix of training data:\\n', confusion_matrix(train_labels_t, train_prediction))\n",
    "\n",
    "print('Classification report of validation data:\\n', classification_report(valid_labels, valid_prediction))\n",
    "print('Confusion Matrix of validation data:\\n', confusion_matrix(valid_labels, valid_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
