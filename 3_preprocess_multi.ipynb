{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "last_percent_reported = None\n",
    "\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"\n",
    "    A hook to report the progress of a download. This is mostly intended for users with\n",
    "    slow internet connections. Reports every 1% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        last_percent_reported = percent\n",
    "        \n",
    "        \n",
    "def maybe_download(filename, force=False):\n",
    "    \"\"\"\n",
    "    Download a file if not present, and make sure it's the right size.\n",
    "    \"\"\"\n",
    "    if force or not os.path.exists(filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    else:\n",
    "        print(filename, 'is already downloaded. Skipped.')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.tar.gz is already downloaded. Skipped.\n"
     ]
    }
   ],
   "source": [
    "train_filename = maybe_download('train.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.tar.gz is already downloaded. Skipped.\n"
     ]
    }
   ],
   "source": [
    "test_filename = maybe_download('test.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra.tar.gz is already downloaded. Skipped.\n"
     ]
    }
   ],
   "source": [
    "extra_filename = maybe_download('extra.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(133)\n",
    "\n",
    "\n",
    "def maybe_extract(file_, force=False):\n",
    "    filename = os.path.splitext(os.path.splitext(file_)[0])[0]  # remove .tar.gz\n",
    "    \n",
    "    if os.path.isdir(filename) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s is already presented - Skipping extraction of %s.' % (filename, file_))\n",
    "    else:\n",
    "        print('Extracting %s file data. Please wait...' % file_)\n",
    "        tar = tarfile.open(file_)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        print('File %s is successfully extracted into %s directory.' % (file_, filename))        \n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train is already presented - Skipping extraction of train.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "# Variables contain directory names where data is extracted\n",
    "train_folders = maybe_extract(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test is already presented - Skipping extraction of test.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra is already presented - Skipping extraction of extra.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "extra_folders = maybe_extract(extra_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_anomaly_samples(data, max_class_length = 5):\n",
    "    \"\"\"\n",
    "    Here we remove all data which has class length higher than specified value.\n",
    "    \"\"\"\n",
    "    print(\"\\nDataset size before update:\", len(data))\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if i < len(data) and len(data[i]['label']) > max_class_length:\n",
    "            print(\"\\nAnomaly at index %d detected. Class size: %d\" % (i, len(data[i]['label'])))\n",
    "            del data[i]\n",
    "            \n",
    "    print(\"\\nDataset after before update:\", len(data))            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# The DigitStructFile is just a wrapper around the h5py data.  It basically references \n",
    "#     file_:            The input h5 matlab file\n",
    "#     digitStructName   The h5 ref to all the file names\n",
    "#     digitStructBbox   The h5 ref to all struc data\n",
    "class DigitStructsWrapper:\n",
    "    def __init__(self, file_):\n",
    "        self.file_ = h5py.File(file_, 'r')\n",
    "        self.names = self.file_['digitStruct']['name']\n",
    "        self.bboxes = self.file_['digitStruct']['bbox']\n",
    "        self.collectionSize = len(self.names)\n",
    "        print(\"\\n%s file structure contain %d entries\" % (file_, self.collectionSize))\n",
    "        \n",
    "        \n",
    "    def bboxHelper(self, keys_):\n",
    "        \"\"\"\n",
    "        Method handles the coding difference when there is exactly one bbox or an array of bbox. \n",
    "        \"\"\"\n",
    "        if (len(keys_) > 1):\n",
    "            val = [self.file_[keys_.value[j].item()].value[0][0] for j in range(len(keys_))]\n",
    "        else:\n",
    "            val = [keys_.value[0][0]]\n",
    "        return val\n",
    "\n",
    "    \n",
    "    # getBbox returns a dict of data for the n(th) bbox. \n",
    "    def getBbox(self, n):\n",
    "        bbox = {}\n",
    "        bb = self.bboxes[n].item()\n",
    "        bbox['height'] = self.bboxHelper(self.file_[bb][\"height\"])\n",
    "        bbox['left'] = self.bboxHelper(self.file_[bb][\"left\"])\n",
    "        bbox['top'] = self.bboxHelper(self.file_[bb][\"top\"])\n",
    "        bbox['width'] = self.bboxHelper(self.file_[bb][\"width\"])\n",
    "        bbox['label'] = self.bboxHelper(self.file_[bb][\"label\"])\n",
    "        return bbox\n",
    "\n",
    "    \n",
    "    def getName(self, n):\n",
    "        \"\"\"\n",
    "        Method returns the filename for the n(th) digitStruct. Since each letter is stored in a structure \n",
    "        as array of ANSII char numbers we should convert it back by calling chr function.\n",
    "        \"\"\"\n",
    "        return ''.join([chr(c[0]) for c in self.file_[self.names[n][0]].value])\n",
    "\n",
    "    \n",
    "    def getNumberStructure(self,n):\n",
    "        s = self.getBbox(n)\n",
    "        s['name']=self.getName(n)\n",
    "        return s\n",
    "\n",
    "    def getAllNumbersStructure(self):\n",
    "        \"\"\"\n",
    "        Method returns an array, which contains information about every image.\n",
    "        This info contains: positions, labels \n",
    "        \"\"\"\n",
    "        return [self.getNumberStructure(i) for i in range(self.collectionSize)]\n",
    "\n",
    "    \n",
    "    # Return a restructured version of the dataset (one object per digit in 'boxes').\n",
    "    #\n",
    "    #   Return a list of dicts :\n",
    "    #      'filename' : filename of the samples\n",
    "    #      'boxes' : list of dicts (one by digit) :\n",
    "    #          'label' : 1 to 9 corresponding digits. 10 for digit '0' in image.\n",
    "    #          'left', 'top' : position of bounding box\n",
    "    #          'width', 'height' : dimension of bounding box\n",
    "    #\n",
    "    # Note: We may turn this to a generator, if memory issues arise.\n",
    "    def getAllNumbersRestructured(self): # getAllDigitStructure_ByDigit\n",
    "        numbersData = self.getAllNumbersStructure()\n",
    "        print(\"\\nObject structure before transforming: \", numbersData[0])\n",
    "        remove_anomaly_samples(numbersData)\n",
    "        \n",
    "        result = []\n",
    "        for numData in numbersData:\n",
    "            metadatas = []\n",
    "            for i in range(len(numData['height'])):\n",
    "                metadata = {}\n",
    "                metadata['height'] = numData['height'][i]\n",
    "                metadata['label']  = numData['label'][i]\n",
    "                metadata['left']   = numData['left'][i]\n",
    "                metadata['top']    = numData['top'][i]\n",
    "                metadata['width']  = numData['width'][i]\n",
    "                metadatas.append(metadata)\n",
    "                \n",
    "            result.append({ 'boxes':metadatas, 'name':numData[\"name\"] })\n",
    "            \n",
    "        print(\"\\nObject structure after transforming: \", result[0])\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train/digitStruct.mat file structure contain 33402 entries\n",
      "\n",
      "Object structure before transforming:  {'name': '1.png', 'top': [77.0, 81.0], 'label': [1.0, 9.0], 'width': [81.0, 96.0], 'height': [219.0, 219.0], 'left': [246.0, 323.0]}\n",
      "\n",
      "Dataset size before update: 33402\n",
      "\n",
      "Anomaly at index 29929 detected. Class size: 6\n",
      "\n",
      "Dataset after before update: 33401\n",
      "\n",
      "Object structure after transforming:  {'boxes': [{'width': 81.0, 'top': 77.0, 'label': 1.0, 'left': 246.0, 'height': 219.0}, {'width': 96.0, 'top': 81.0, 'label': 9.0, 'left': 323.0, 'height': 219.0}], 'name': '1.png'}\n"
     ]
    }
   ],
   "source": [
    "train_folders = 'train'\n",
    "\n",
    "file_ = os.path.join(train_folders, 'digitStruct.mat')\n",
    "dsf = DigitStructsWrapper(file_)\n",
    "train_data = dsf.getAllNumbersRestructured()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test/digitStruct.mat file structure contain 13068 entries\n",
      "\n",
      "Object structure before transforming:  {'name': '1.png', 'top': [7.0], 'label': [5.0], 'width': [19.0], 'height': [30.0], 'left': [43.0]}\n",
      "\n",
      "Dataset size before update: 13068\n",
      "\n",
      "Dataset after before update: 13068\n",
      "\n",
      "Object structure after transforming:  {'boxes': [{'width': 19.0, 'top': 7.0, 'label': 5.0, 'left': 43.0, 'height': 30.0}], 'name': '1.png'}\n"
     ]
    }
   ],
   "source": [
    "test_folders = 'test'\n",
    "\n",
    "file_ = os.path.join(test_folders, 'digitStruct.mat')\n",
    "dsf = DigitStructsWrapper(file_)\n",
    "test_data = dsf.getAllNumbersRestructured()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876.0 501.0\n",
      "25.0 12.0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "train_imgSize = np.ndarray([len(train_data),2])\n",
    "\n",
    "for i in np.arange(len(train_data)):\n",
    "    filename = train_data[i]['name']\n",
    "    filepath = os.path.join(train_folders, filename)\n",
    "    train_imgSize[i, :] = Image.open(filepath).size[:]\n",
    "\n",
    "train_max_width = np.amax(train_imgSize[:,0])\n",
    "train_max_height = np.amax(train_imgSize[:,1])\n",
    "print(train_max_width, train_max_height)\n",
    "\n",
    "train_min_width = np.amin(train_imgSize[:,0])    \n",
    "train_min_height = np.amin(train_imgSize[:,1])    \n",
    "print(train_min_width, train_min_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1083.0 516.0\n",
      "31.0 13.0\n"
     ]
    }
   ],
   "source": [
    "test_imgSize = np.ndarray([len(test_data),2])\n",
    "\n",
    "for i in np.arange(len(test_data)):\n",
    "    filename = test_data[i]['name']\n",
    "    filepath = os.path.join(test_folders, filename)\n",
    "    test_imgSize[i, :] = Image.open(filepath).size[:]\n",
    "\n",
    "test_max_width = np.amax(test_imgSize[:,0])\n",
    "test_max_height = np.amax(test_imgSize[:,1])\n",
    "print(test_max_width, test_max_height)\n",
    "\n",
    "test_min_width = np.amin(test_imgSize[:,0])    \n",
    "test_min_height = np.amin(test_imgSize[:,1])    \n",
    "print(test_min_width, test_min_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  410,  4163, 15855, 30483]),)\n",
      "(array([9747]),)\n",
      "(array([ 1722,  2949,  6233, 12862]),)\n",
      "(array([  459,  5352,  7776, 11257, 12191]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.where(train_imgSize[:,0]==train_max_width))\n",
    "print(np.where(train_imgSize[:,0]==train_min_width))\n",
    "print(np.where(test_imgSize[:,0]==test_max_width))\n",
    "print(np.where(test_imgSize[:,0]==test_min_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "\n",
    "def prepare_images(samples, folder):\n",
    "    print(\"Started preparing images for convnet...\")\n",
    "    \n",
    "    prepared_images = np.ndarray([len(samples),img_size,img_size,1], dtype='float32')\n",
    "    actual_numbers = np.ones([len(samples),6], dtype=int) * 0\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        filename = samples[i]['name']\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        image = Image.open(filepath)\n",
    "        boxes = samples[i]['boxes']\n",
    "        number_length = len(boxes)\n",
    "        \n",
    "        # at 0 index we store length of a label. 3 -> 1; 123-> 3, 12543 -> 5\n",
    "        actual_numbers[i,0] = number_length\n",
    "        \n",
    "        top = np.ndarray([number_length], dtype='float32')\n",
    "        left = np.ndarray([number_length], dtype='float32')\n",
    "        height = np.ndarray([number_length], dtype='float32')\n",
    "        width = np.ndarray([number_length], dtype='float32')\n",
    "        \n",
    "        for j in range(number_length):\n",
    "            # here we use j+1 since first entry used by label length\n",
    "            actual_numbers[i,j+1] = boxes[j]['label']\n",
    "            if boxes[j]['label'] == 10: # Replacing 10 with 0\n",
    "                actual_numbers[i,j+1] = 0\n",
    "                \n",
    "            top[j] = boxes[j]['top']\n",
    "            left[j] = boxes[j]['left']\n",
    "            height[j] = boxes[j]['height']\n",
    "            width[j] = boxes[j]['width']\n",
    "        \n",
    "        img_min_top = np.amin(top)\n",
    "        img_min_left = np.amin(left)\n",
    "        img_height = np.amax(top) + height[np.argmax(top)] - img_min_top\n",
    "        img_width = np.amax(left) + width[np.argmax(left)] - img_min_left\n",
    "\n",
    "        img_left = np.floor(img_min_left - 0.1 * img_width)\n",
    "        img_top = np.floor(img_min_top - 0.1 * img_height)\n",
    "        img_right = np.amin([np.ceil(img_left + 1.2 * img_width), image.size[0]])\n",
    "        img_bottom = np.amin([np.ceil(img_top + 1.2 * img_height), image.size[1]])\n",
    "            \n",
    "        image = image.crop((img_left, img_top, img_right, img_bottom)).resize([img_size, img_size], Image.ANTIALIAS) # Resize image to 32x32\n",
    "        image = np.dot(np.array(image, dtype='float32'), [[0.2989],[0.5870],[0.1140]]) # Convert image to the grayscale\n",
    "        if i == 0:\n",
    "            print(image.shape)\n",
    "        mean = np.mean(image, dtype='float32')\n",
    "        std = np.std(image, dtype='float32', ddof=1)\n",
    "        if std < 0.0001: \n",
    "            std = 1.0\n",
    "        image = (image - mean) / std\n",
    "        prepared_images[i,:,:] = image[:,:,:]\n",
    "        \n",
    "    print(\"Completed. Images cropped, resized and grayscaled\")\n",
    "    \n",
    "    return prepared_images, actual_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started preparing images for convnet...\n",
      "(32, 32, 1)\n",
      "Completed. Images cropped, resized and grayscaled\n",
      "(33401, 32, 32, 1)\n",
      "(33401, 6)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_labels = prepare_images(train_data, train_folders)\n",
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started preparing images for convnet...\n",
      "(32, 32, 1)\n",
      "Completed. Images cropped, resized and grayscaled\n",
      "(13068, 32, 32, 1) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "test_dataset, test_labels = prepare_images(test_data, test_folders)\n",
    "print(test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_dataset, train_labels = shuffle(train_dataset, train_labels)\n",
    "test_dataset, test_labels = shuffle(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 192567884\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN_multi.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "        }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
