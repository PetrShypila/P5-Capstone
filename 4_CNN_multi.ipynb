{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (22378, 32, 32, 1) (22378, 6)\n",
      "Validation set (11023, 32, 32, 1) (11023, 6)\n",
      "Test set (13068, 32, 32, 1) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN_multi.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "    train_dataset, valid_dataset, train_labels, valid_labels = train_test_split(train_dataset, train_labels, test_size=0.33)\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_filter(kernel_shape):\n",
    "    x = np.zeros(kernel_shape, dtype = float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    \n",
    "    for kernel_idx in xrange(0, kernel_shape[2]):\n",
    "        for i in xrange(0, kernel_shape[0]):\n",
    "            for j in xrange(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gauss(i - mid, j - mid)\n",
    "    \n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss(x, y, sigma=3.0):\n",
    "    Z = 2 * np.pi * sigma ** 2\n",
    "    return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LecunLCN(X, image_shape, threshold=1e-4, radius=7, use_divisor=True):\n",
    "    \"\"\"Local Contrast Normalization\"\"\"\n",
    "    \"\"\"[http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf]\"\"\"\n",
    "\n",
    "    # Get Gaussian filter\n",
    "    filter_shape = (radius, radius, image_shape[3], 1)\n",
    "\n",
    "    #self.filters = theano.shared(self.gaussian_filter(filter_shape), borrow=True)\n",
    "    filters = gaussian_filter(filter_shape)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    # Compute the Guassian weighted average by means of convolution\n",
    "    convout = tf.nn.conv2d(X, filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "    # Subtractive step\n",
    "    mid = int(np.floor(filter_shape[1] / 2.))\n",
    "\n",
    "    # Make filter dimension broadcastable and subtract\n",
    "    centered_X = tf.sub(X, convout)\n",
    "\n",
    "    # Boolean marks whether or not to perform divisive step\n",
    "    if use_divisor:\n",
    "        # Note that the local variances can be computed by using the centered_X\n",
    "        # tensor. If we convolve this with the mean filter, that should give us\n",
    "        # the variance at each point. We simply take the square root to get our\n",
    "        # denominator\n",
    "\n",
    "        # Compute variances\n",
    "        sum_sqr_XX = tf.nn.conv2d(tf.square(centered_X), filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "        # Take square root to get local standard deviation\n",
    "        denom = tf.sqrt(sum_sqr_XX)\n",
    "\n",
    "        per_img_mean = tf.reduce_mean(denom)\n",
    "        divisor = tf.maximum(per_img_mean, denom)\n",
    "        # Divisise step\n",
    "        new_X = tf.truediv(centered_X, tf.maximum(divisor, threshold))\n",
    "    else:\n",
    "        new_X = centered_X\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels, printstat=False):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden1 = 64\n",
    "shape = [batch_size, image_size, image_size, num_channels]\n",
    "\n",
    "# Construct a 7-layer CNN.\n",
    "# C1: convolutional layer, batch_size x 28 x 28 x 16, convolution size: 5 x 5 x 1 x 16\n",
    "# P2: pooling layer, batch_size x 14 x 14 x 16\n",
    "# C3: convolutional layer, batch_size x 10 x 10 x 32, convolution size: 5 x 5 x 16 x 32\n",
    "# P4: pooling layer, batch_size x 5 x 5 x 32\n",
    "# C5: convolutional layer, batch_size x 1 x 1 x 64, convolution size: 5 x 5 x 32 x 64\n",
    "# Dropout\n",
    "# F6: fully-connected layer, weight size: 64 x 16\n",
    "# Output layer, weight size: 16 x 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data placeholders.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.get_variable(\"W1\", shape=[patch_size, patch_size, num_channels, depth1],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth1]), name='B1')\n",
    "    \n",
    "    layer2_weights = tf.get_variable(\"W2\", shape=[patch_size, patch_size, depth1, depth2],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]), name='B2')\n",
    "    \n",
    "    layer3_weights = tf.get_variable(\"W3\", shape=[patch_size, patch_size, depth2, num_hidden1],\\\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden1]), name='B3')\n",
    "\n",
    "    s1_w = tf.get_variable(\"WS1\", shape=[num_hidden1, num_labels],\\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s1_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS1')\n",
    "    \n",
    "    s2_w = tf.get_variable(\"WS2\", shape=[num_hidden1, num_labels],\\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s2_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS2')\n",
    "    \n",
    "    s3_w = tf.get_variable(\"WS3\", shape=[num_hidden1, num_labels],\\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s3_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS3')\n",
    "    \n",
    "    s4_w = tf.get_variable(\"WS4\", shape=[num_hidden1, num_labels],\\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s4_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS4')\n",
    "    \n",
    "    s5_w = tf.get_variable(\"WS5\", shape=[num_hidden1, num_labels],\\\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    s5_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS5')\n",
    "\n",
    "    # Model.\n",
    "    def model(data, keep_prob, shape):\n",
    "        LCN = LecunLCN(data, shape)\n",
    "        \n",
    "        conv = tf.nn.conv2d(LCN, layer1_weights, [1,1,1,1], 'VALID', name='C1')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        lrn = tf.nn.local_response_normalization(hidden)\n",
    "        sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='P2')\n",
    "        \n",
    "        conv = tf.nn.conv2d(sub, layer2_weights, [1,1,1,1], padding='VALID', name='C3')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        lrn = tf.nn.local_response_normalization(hidden)\n",
    "        sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='P4')\n",
    "        \n",
    "        conv = tf.nn.conv2d(sub, layer3_weights, [1,1,1,1], padding='VALID', name='C5')        \n",
    "        hidden = tf.nn.relu(conv + layer3_biases)\n",
    "        hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "        \n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        logits1 = tf.matmul(reshape, s1_w) + s1_b\n",
    "        logits2 = tf.matmul(reshape, s2_w) + s2_b\n",
    "        logits3 = tf.matmul(reshape, s3_w) + s3_b\n",
    "        logits4 = tf.matmul(reshape, s4_w) + s4_b\n",
    "        logits5 = tf.matmul(reshape, s5_w) + s5_b\n",
    "        \n",
    "        return [logits1, logits2, logits3, logits4, logits5]\n",
    "\n",
    "    # Training computation.\n",
    "    [logits1, logits2, logits3, logits4, logits5] = model(tf_train_dataset, 0.9375, shape)\n",
    "    loss =  tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_train_labels[:,1])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_train_labels[:,2])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_train_labels[:,3])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_train_labels[:,4])) +\\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_train_labels[:,5]))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation and test data.\n",
    "    train_logits = model(tf_train_dataset, 1.0, shape)\n",
    "    train_prediction = tf.pack([tf.nn.softmax(train_logits[0]), tf.nn.softmax(train_logits[1]),\\\n",
    "                                tf.nn.softmax(train_logits[2]), tf.nn.softmax(train_logits[3]),\\\n",
    "                                tf.nn.softmax(train_logits[4])])\n",
    "    \n",
    "    valid_logits = model(tf_valid_dataset, 1.0, shape)\n",
    "    valid_prediction = tf.pack([tf.nn.softmax(valid_logits[0]), tf.nn.softmax(valid_logits[1]),\\\n",
    "                                tf.nn.softmax(valid_logits[2]), tf.nn.softmax(valid_logits[3]),\\\n",
    "                                tf.nn.softmax(valid_logits[4])])\n",
    "    \n",
    "    test_logits = model(tf_test_dataset, 1.0, shape)\n",
    "    test_prediction = tf.pack([tf.nn.softmax(test_logits[0]), tf.nn.softmax(test_logits[1]),\\\n",
    "                               tf.nn.softmax(test_logits[2]), tf.nn.softmax(test_logits[3]),\\\n",
    "                               tf.nn.softmax(test_logits[4])])\n",
    "\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.073668\n",
      "Minibatch accuracy: 5.0%\n",
      "Validation accuracy: 61.2%\n",
      "Minibatch loss at step 1000: 3.457461\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 2.005269\n",
      "predictions [[  1.33394357e-03   3.11315339e-02   1.12315384e-03   1.31169688e-02\n",
      "    6.63131326e-02   2.80471258e-02   8.40821266e-01   1.03937078e-03\n",
      "    1.47115691e-02   2.36073672e-03   1.24429778e-06]\n",
      " [  2.08278326e-03   6.78801388e-02   2.06011087e-02   6.58271790e-01\n",
      "    2.39957450e-03   1.68343201e-01   2.43638493e-02   4.02428471e-02\n",
      "    8.75692349e-03   7.05719553e-03   5.74274111e-07]\n",
      " [  1.88893836e-03   2.71043717e-03   4.00113799e-02   7.93708563e-01\n",
      "    1.82327465e-03   5.53105026e-02   7.57606467e-03   3.02171055e-03\n",
      "    8.43409821e-02   9.58402827e-03   2.42399128e-05]\n",
      " [  7.82999996e-05   4.42824699e-03   9.82135832e-01   8.54563899e-03\n",
      "    7.29546417e-04   7.81697687e-04   1.63347839e-04   1.99527084e-03\n",
      "    8.55890103e-04   2.74673192e-04   1.16160118e-05]\n",
      " [  1.55492534e-03   5.36756873e-01   1.72113255e-01   2.29406983e-01\n",
      "    1.48537885e-02   3.24507896e-03   3.48981004e-03   1.12192398e-02\n",
      "    1.28148841e-02   1.45119149e-02   3.31292977e-05]\n",
      " [  9.20399092e-04   1.21929171e-02   9.16514400e-05   9.21056196e-02\n",
      "    6.31184783e-03   8.24661314e-01   5.57678267e-02   2.64015660e-04\n",
      "    1.08521827e-03   6.59903279e-03   5.63568641e-08]\n",
      " [  1.39232399e-03   6.79608854e-03   2.05625370e-02   9.26823795e-01\n",
      "    5.48630720e-04   2.37983149e-02   5.37323486e-03   4.67535527e-03\n",
      "    7.77414767e-03   1.73775724e-03   5.17703244e-04]\n",
      " [  2.54683243e-03   7.15058222e-02   8.27350140e-01   5.06100710e-03\n",
      "    4.34476358e-04   1.09716821e-04   9.36229597e-04   8.57644826e-02\n",
      "    4.66862507e-03   1.62207650e-03   6.80593246e-07]\n",
      " [  4.90807230e-04   9.73511219e-01   2.08470761e-03   5.39241126e-04\n",
      "    1.15359128e-02   1.98993250e-04   7.68130273e-03   9.17445868e-04\n",
      "    2.56164069e-03   4.78170754e-04   4.92296238e-07]\n",
      " [  2.05027289e-03   8.92114103e-01   2.17821356e-02   9.13004740e-04\n",
      "    4.77990732e-02   1.13785325e-03   1.78125016e-02   6.01952476e-03\n",
      "    8.01517535e-03   1.59670017e-03   7.59671442e-04]\n",
      " [  3.17654456e-04   1.01664789e-01   5.78310013e-01   5.38530611e-02\n",
      "    5.76660794e-04   1.78191555e-03   9.05317604e-04   2.55360246e-01\n",
      "    5.21755219e-03   2.01184768e-03   9.78799108e-07]\n",
      " [  2.68282625e-03   8.63424778e-01   2.87901983e-03   5.39602675e-02\n",
      "    5.98987285e-03   1.25434874e-02   1.45752886e-02   3.59479226e-02\n",
      "    4.56098514e-03   3.43335117e-03   2.29656871e-06]\n",
      " [  1.79742292e-05   9.80351865e-01   1.02873221e-02   4.72085690e-03\n",
      "    3.75527947e-04   5.12557999e-05   2.52686703e-04   2.89780530e-03\n",
      "    1.00926915e-03   3.51923700e-05   2.16817810e-07]\n",
      " [  2.09633072e-05   8.63799360e-04   5.41343645e-04   8.36139143e-01\n",
      "    3.67688626e-04   1.57804891e-01   1.76536757e-03   6.99202530e-04\n",
      "    9.23982647e-04   8.73168872e-04   3.49492922e-07]\n",
      " [  3.95799102e-03   3.69121224e-01   1.17446460e-01   1.20036509e-02\n",
      "    1.03455363e-02   5.70602773e-04   5.23016937e-02   1.04781203e-02\n",
      "    4.11236227e-01   1.25137335e-02   2.46775944e-05]\n",
      " [  8.67623184e-03   6.91259876e-02   7.37792030e-02   1.61016479e-01\n",
      "    2.15560403e-02   3.61681581e-01   1.12656467e-01   5.53863905e-02\n",
      "    1.18544437e-01   1.75505802e-02   2.66623902e-05]\n",
      " [  2.71813874e-03   7.67234191e-02   4.31904523e-03   2.93502301e-01\n",
      "    1.94362514e-02   2.84233749e-01   2.80037463e-01   9.13318805e-03\n",
      "    2.77983844e-02   1.65309862e-03   4.44890000e-04]\n",
      " [  4.57711605e-04   1.59889795e-02   2.38702793e-04   1.16286539e-02\n",
      "    3.63433210e-04   5.59149273e-02   9.10769701e-01   6.64495223e-04\n",
      "    3.80167458e-03   1.22359721e-04   4.93553125e-05]\n",
      " [  8.28181437e-05   1.07699900e-03   9.87472892e-01   9.37133795e-04\n",
      "    8.33928643e-04   1.33607537e-05   5.55292427e-05   1.64235511e-03\n",
      "    6.39863173e-03   1.48610561e-03   7.04052141e-08]\n",
      " [  3.04129068e-03   1.82163700e-01   7.04741120e-01   8.80851224e-03\n",
      "    5.73431738e-02   1.59967528e-03   5.54720266e-03   2.00357139e-02\n",
      "    1.32879112e-02   1.80835032e-03   1.62346091e-03]\n",
      " [  4.48189769e-03   8.83318707e-02   8.74756544e-04   7.22898304e-01\n",
      "    4.49627638e-03   1.07502870e-01   5.55403084e-02   6.88680820e-03\n",
      "    6.02684729e-03   2.95951171e-03   5.61684544e-07]\n",
      " [  4.45303554e-03   1.40180811e-01   2.11252376e-01   3.91560458e-02\n",
      "    8.67097918e-03   3.27556371e-03   5.73682860e-02   2.00653300e-02\n",
      "    4.97113287e-01   1.82939507e-02   1.70370360e-04]\n",
      " [  3.16599326e-04   9.74018395e-01   7.75325578e-04   1.66224345e-04\n",
      "    3.55154800e-04   1.67925566e-04   2.27587782e-02   2.29189522e-04\n",
      "    7.33150868e-04   4.10210196e-04   6.89875305e-05]\n",
      " [  8.86286143e-03   1.93025041e-02   6.53794734e-03   5.16287461e-02\n",
      "    2.11064629e-02   7.35634938e-02   3.48367125e-01   1.50446504e-04\n",
      "    4.40952212e-01   2.95251478e-02   3.02163721e-06]\n",
      " [  1.91360799e-04   7.14795850e-03   3.70649286e-06   3.80359794e-04\n",
      "    4.17951487e-05   4.47825827e-02   9.44871426e-01   2.71364115e-05\n",
      "    2.32877885e-03   2.19046487e-04   5.86311717e-06]\n",
      " [  3.42658907e-03   4.72697765e-01   1.41594997e-02   3.59459758e-01\n",
      "    8.11209604e-02   3.11464891e-02   7.65169924e-03   1.48792025e-02\n",
      "    6.28785836e-03   9.16974340e-03   4.79764878e-07]\n",
      " [  1.61717627e-02   2.73250878e-01   4.64765728e-03   2.57374823e-01\n",
      "    3.35487910e-02   1.45939738e-01   2.31059775e-01   4.51118778e-03\n",
      "    2.66295280e-02   6.76364126e-03   1.02209116e-04]\n",
      " [  9.08443144e-06   9.98891771e-01   6.05557125e-06   3.98354532e-05\n",
      "    1.37931725e-04   1.57867435e-05   5.49955061e-04   2.21792026e-04\n",
      "    1.23291495e-04   4.48257424e-06   4.14929460e-08]\n",
      " [  3.21712135e-03   5.87764859e-01   2.74977267e-01   3.97752970e-03\n",
      "    4.51531038e-02   4.53633629e-03   2.76073031e-02   2.28003552e-03\n",
      "    1.66267194e-02   3.38565707e-02   3.16766250e-06]\n",
      " [  6.44332450e-03   8.67130101e-01   1.20535903e-02   5.32484287e-03\n",
      "    4.19854681e-04   5.61057124e-04   5.01126796e-02   2.20371652e-02\n",
      "    3.26747969e-02   3.16993683e-03   7.26092694e-05]\n",
      " [  1.93086933e-04   9.88773942e-01   1.40771584e-03   2.79859919e-03\n",
      "    2.33913888e-04   2.43638569e-05   1.15998700e-04   6.00767229e-03\n",
      "    3.64737643e-04   7.90338090e-05   9.33282706e-07]\n",
      " [  5.48859476e-04   2.69058291e-02   9.38585997e-02   8.35153997e-01\n",
      "    2.09750235e-03   5.78434346e-03   9.33307165e-04   9.75483190e-03\n",
      "    7.16297049e-03   1.77728478e-02   2.70998535e-05]\n",
      " [  5.85456321e-04   1.09003074e-01   2.21720506e-02   1.38145238e-01\n",
      "    1.42636020e-02   5.07073641e-01   1.02808788e-01   1.32195710e-03\n",
      "    5.78589216e-02   4.66999821e-02   6.73661707e-05]\n",
      " [  2.06601247e-03   9.81490612e-02   7.21299767e-01   1.16392508e-01\n",
      "    1.78597383e-02   7.58228917e-03   1.72965310e-03   2.64933985e-02\n",
      "    3.89394141e-03   4.49499721e-03   3.86894062e-05]\n",
      " [  2.18233821e-04   2.90729484e-04   2.05966862e-04   5.19581186e-03\n",
      "    1.70820291e-04   9.36941504e-01   5.53261712e-02   1.21508056e-04\n",
      "    1.04000012e-03   4.89071186e-04   1.85612691e-07]\n",
      " [  1.76003596e-05   4.25442595e-05   9.98658657e-01   2.17648831e-05\n",
      "    1.25849374e-05   1.00965678e-06   2.33394576e-06   8.04261595e-04\n",
      "    3.86110914e-04   5.31140759e-05   2.41020413e-08]\n",
      " [  1.66380734e-04   4.41031822e-04   9.40457321e-05   2.71322504e-02\n",
      "    1.67241684e-04   8.45015228e-01   8.82875770e-02   3.70109774e-05\n",
      "    1.67716555e-02   2.18861047e-02   1.54952909e-06]\n",
      " [  1.67452358e-03   7.44780540e-01   1.75130051e-02   2.68905200e-02\n",
      "    2.76036980e-03   1.80462294e-03   3.85857783e-02   7.97293149e-03\n",
      "    1.55019447e-01   2.61959922e-03   3.78684956e-04]\n",
      " [  4.83020587e-04   9.61592495e-01   5.45106875e-03   3.47548700e-03\n",
      "    1.80701993e-03   6.24907028e-04   1.03332680e-02   4.69505647e-03\n",
      "    1.05695007e-02   1.61538934e-04   8.06652417e-04]\n",
      " [  7.35094136e-06   1.17075839e-03   1.82651638e-04   1.13253249e-03\n",
      "    9.93985593e-01   1.23129645e-03   2.77841988e-04   1.12947992e-05\n",
      "    3.17829719e-04   1.68297801e-03   6.50038912e-09]\n",
      " [  9.21628438e-04   4.94727008e-02   2.73918326e-04   1.29095037e-02\n",
      "    8.22038054e-01   9.12021250e-02   1.68194249e-02   8.37519256e-05\n",
      "    3.98317992e-04   5.87986596e-03   7.59037164e-07]\n",
      " [  1.52039458e-03   1.57678649e-01   2.31470130e-02   3.88432071e-02\n",
      "    4.28000814e-04   1.39805777e-02   4.55119461e-01   1.25101497e-02\n",
      "    2.91937888e-01   4.30011703e-03   5.34485211e-04]\n",
      " [  4.54828143e-04   8.45927954e-01   4.63935779e-03   2.86079990e-03\n",
      "    1.21104091e-01   7.37834896e-04   1.85981058e-02   6.56771183e-04\n",
      "    4.14779736e-03   8.57388310e-04   1.52704506e-05]\n",
      " [  2.52520877e-05   7.69704301e-03   1.02438231e-03   2.37174478e-04\n",
      "    9.89555478e-01   5.68322102e-05   7.41196040e-04   6.83365506e-05\n",
      "    4.74603497e-04   1.19813652e-04   2.54282444e-08]\n",
      " [  1.45556944e-04   9.90880489e-01   4.99315734e-04   2.77227984e-04\n",
      "    5.09594288e-03   3.28566312e-05   1.72114337e-03   6.49638183e-04\n",
      "    5.08000783e-04   1.89847662e-04   2.26212897e-08]\n",
      " [  5.51172474e-04   1.93887800e-02   4.78844624e-03   1.14617171e-02\n",
      "    8.67803633e-01   1.45606715e-02   3.04132793e-02   1.19372271e-04\n",
      "    1.51923615e-02   3.57204676e-02   2.25745595e-07]\n",
      " [  2.52099172e-03   7.77869523e-01   1.58973739e-01   1.56326741e-02\n",
      "    1.23716602e-02   2.90216273e-03   7.07932748e-03   7.98235927e-03\n",
      "    6.22724509e-03   7.95853790e-03   4.81770578e-04]\n",
      " [  5.05898288e-03   5.65044023e-02   6.39379382e-01   1.07306428e-01\n",
      "    4.13168978e-04   4.47402745e-02   1.54502364e-02   5.67103587e-02\n",
      "    2.34000068e-02   4.27709036e-02   8.26589018e-03]\n",
      " [  3.22147680e-04   5.04053310e-02   8.39768816e-03   9.06926155e-01\n",
      "    4.52354737e-03   1.89661290e-02   3.24396230e-03   6.01551589e-03\n",
      "    9.49868176e-04   2.42143506e-04   7.41627764e-06]\n",
      " [  3.71570204e-04   5.51666832e-03   9.90280509e-01   2.46345066e-04\n",
      "    4.94257023e-04   1.70016192e-05   3.78255390e-05   8.47637828e-04\n",
      "    6.23738219e-04   1.55852945e-03   5.88469811e-06]\n",
      " [  4.97858971e-04   3.33527895e-03   9.73116219e-01   5.08293851e-05\n",
      "    3.08131654e-04   2.15470868e-06   3.96735180e-04   1.53319426e-02\n",
      "    6.60612062e-03   3.53568961e-04   1.12278212e-06]\n",
      " [  9.13819298e-04   2.15194654e-02   1.17438822e-03   9.52407360e-01\n",
      "    1.59307651e-03   8.33388790e-03   8.47827643e-04   8.31929222e-03\n",
      "    3.09457281e-03   1.79505139e-03   1.31918659e-06]\n",
      " [  8.22049193e-03   2.09185898e-01   3.66721489e-02   5.78623824e-03\n",
      "    4.29026142e-04   3.36322963e-04   6.50097197e-03   7.26399481e-01\n",
      "    4.42471821e-03   2.04417459e-03   4.66645247e-07]\n",
      " [  5.76395250e-04   5.81814442e-04   8.89677942e-01   2.89945155e-02\n",
      "    2.16892199e-03   2.28618184e-04   2.15176318e-04   8.72313138e-03\n",
      "    4.50151898e-02   2.38176119e-02   4.89674051e-07]\n",
      " [  1.96495699e-03   2.61904597e-02   9.23247755e-01   8.71291943e-03\n",
      "    2.61828750e-02   2.98398838e-04   3.75097501e-04   1.50981033e-03\n",
      "    2.88424129e-03   8.63354746e-03   1.11975723e-07]\n",
      " [  1.14294223e-03   6.95502833e-02   6.15260052e-03   4.68009757e-03\n",
      "    1.60938618e-03   4.59141302e-04   6.90882187e-03   9.06975687e-01\n",
      "    2.47036363e-03   4.83408512e-05   2.41291991e-06]\n",
      " [  1.12330308e-03   2.46651575e-01   9.61621990e-04   1.14776291e-01\n",
      "    1.71874285e-01   2.26639152e-01   1.41839296e-01   7.40367977e-05\n",
      "    4.63564098e-02   4.97021973e-02   1.87361047e-06]\n",
      " [  4.24945174e-05   9.88999188e-01   5.55505612e-05   4.03484469e-03\n",
      "    8.99924722e-04   2.71837955e-04   2.68143788e-03   6.57755067e-04\n",
      "    2.34693033e-03   1.00433172e-05   1.15250778e-07]\n",
      " [  9.79972654e-04   4.09436449e-02   8.91115010e-01   2.17378885e-02\n",
      "    9.96803399e-03   2.72329152e-03   1.22319034e-03   1.30726155e-02\n",
      "    9.46607813e-03   8.74929503e-03   2.10835569e-05]\n",
      " [  9.16621575e-05   9.80678380e-01   4.56005546e-05   1.77960013e-04\n",
      "    6.17699930e-04   3.51448864e-04   1.69705991e-02   2.87812960e-04\n",
      "    7.30820582e-04   4.68430226e-05   1.20435016e-06]\n",
      " [  1.17711024e-03   1.77132804e-02   5.27378498e-03   1.30080921e-03\n",
      "    9.45082724e-01   2.20931508e-03   2.41890848e-02   5.53073303e-04\n",
      "    2.31625536e-03   1.84289209e-04   2.65253306e-07]\n",
      " [  3.81706515e-03   2.85182577e-02   8.23688880e-02   2.03949865e-02\n",
      "    8.54775071e-01   1.32856029e-03   6.07445894e-04   8.52399622e-04\n",
      "    5.77297295e-04   6.75916113e-03   9.87697149e-07]\n",
      " [  2.69055568e-06   1.08196939e-04   9.98139143e-01   2.19329995e-05\n",
      "    1.67136590e-04   1.30522358e-06   3.67777631e-07   4.25918515e-05\n",
      "    4.23517835e-04   1.09310774e-03   2.26489405e-09]\n",
      " [  9.00753366e-05   9.96234357e-01   2.30658392e-04   2.82929323e-05\n",
      "    1.21405363e-04   2.73173009e-06   1.09735900e-03   1.48382923e-03\n",
      "    6.96097733e-04   1.50790620e-05   3.45695156e-07]]\n",
      "labels [6 3 8 2 3 5 3 2 1 4 2 1 1 3 8 5 5 6 2 2 3 8 1 8 6 1 0 1 2 1 1 3 5 2 5 2 5\n",
      " 1 1 4 4 6 1 4 1 4 2 2 3 2 2 3 7 3 2 7 4 1 2 1 4 4 2 1]\n",
      "np.argmax(predictions, 2).T [[6 4 0 0 0]\n",
      " [3 3 0 0 0]\n",
      " [3 0 0 0 0]\n",
      " [2 5 5 0 0]\n",
      " [1 4 0 0 0]\n",
      " [5 0 0 0 0]\n",
      " [3 9 7 0 0]\n",
      " [2 0 0 0 0]\n",
      " [1 1 0 0 0]\n",
      " [1 4 4 0 0]\n",
      " [2 5 0 0 0]\n",
      " [1 7 0 0 0]\n",
      " [1 6 0 0 0]\n",
      " [3 5 0 0 0]\n",
      " [8 7 0 0 0]\n",
      " [5 4 0 0 0]\n",
      " [3 5 0 0 0]\n",
      " [6 5 9 0 0]\n",
      " [2 0 0 0 0]\n",
      " [2 5 5 0 0]\n",
      " [3 0 0 0 0]\n",
      " [8 4 0 0 0]\n",
      " [1 9 0 0 0]\n",
      " [8 0 0 0 0]\n",
      " [6 9 0 0 0]\n",
      " [1 3 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 8 0 0 0]\n",
      " [1 2 0 0 0]\n",
      " [1 9 0 0 0]\n",
      " [1 7 0 0 0]\n",
      " [3 4 0 0 0]\n",
      " [5 5 0 0 0]\n",
      " [2 4 5 0 0]\n",
      " [5 0 0 0 0]\n",
      " [2 0 0 0 0]\n",
      " [5 2 0 0 0]\n",
      " [1 2 4 0 0]\n",
      " [1 6 4 0 0]\n",
      " [4 3 0 0 0]\n",
      " [4 7 0 0 0]\n",
      " [6 2 2 0 0]\n",
      " [1 4 5 0 0]\n",
      " [4 8 0 0 0]\n",
      " [1 4 0 0 0]\n",
      " [4 5 0 0 0]\n",
      " [1 1 7 0 0]\n",
      " [2 9 5 0 0]\n",
      " [3 3 5 0 0]\n",
      " [2 7 1 0 0]\n",
      " [2 0 0 0 0]\n",
      " [3 7 0 0 0]\n",
      " [7 0 0 0 0]\n",
      " [2 0 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [7 6 0 0 0]\n",
      " [1 1 0 0 0]\n",
      " [1 3 0 0 0]\n",
      " [2 4 4 0 0]\n",
      " [1 5 0 0 0]\n",
      " [4 0 0 0 0]\n",
      " [4 0 0 0 0]\n",
      " [2 3 0 0 0]\n",
      " [1 7 0 0 0]]\n",
      "np.sum(np.argmax(predictions, 2).T == labels) 286\n",
      "predictions.shape (5, 64, 11)\n",
      "labels.shape (64, 5)\n",
      "Acc 89.375\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3000: 2.059390\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4000: 1.797640\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5000: 1.110449\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 6000: 1.119308\n",
      "Minibatch accuracy: 94.7%\n",
      "Validation accuracy: 89.9%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3f5528304cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()  \n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "        if (step % 1000 == 0): \n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,1:6]))    \n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,1:6]))\n",
    "    \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,1:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "f, ax = plt.subplots(nrows=1, ncols=10)\n",
    "\n",
    "im_samples = []\n",
    "    \n",
    "for i, j in enumerate(np.sort(np.random.randint(0, test_labels.shape[0], size=10))):\n",
    "    filename = str(j+1)+'.png'\n",
    "    fullname = os.path.join('test', filename)\n",
    "    im = Image.open(fullname)\n",
    "    house_num = ''\n",
    "    for k in np.arange(test_labels[j,0]):\n",
    "        house_num += str(test_labels[j,k+1])\n",
    "        \n",
    "    im_samples.extend([j])\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(house_num, loc='center')\n",
    "    ax[i].imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
